{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# importing a module with utilities for displaying stats and data\n",
    "import sys\n",
    "sys.path.insert(1, '../../util')\n",
    "import vcpi_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_III(model, train_loader, val_loader, epochs, loss_fn, optimizer, scheduler, early_stopper, save_prefix = 'model'):\n",
    "\n",
    "    history = {}\n",
    "    history['accuracy'] = []\n",
    "    history['val_acc'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['loss'] = []\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        model.train()\n",
    "        start_time = time.time() \n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader, 0):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "            correct += (predicted == targets).sum()\n",
    "\n",
    "        model.eval()\n",
    "        v_correct = 0\n",
    "        val_loss = 0.0\n",
    "        with torch_no_grad():\n",
    "            for i,t in val_loader:\n",
    "                i = i.to(device)\n",
    "                t = t.to(device)\n",
    "                o = model(i)\n",
    "                _,p = torch.max(o,1)\n",
    "                \n",
    "                #with torch.no_grad():\n",
    "                val_loss += loss_fn(o, t)\n",
    "\n",
    "                v_correct += (p == t).sum()\n",
    "\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if old_lr != new_lr:\n",
    "            print('==> Learning rate updated: ', old_lr, ' -> ', new_lr)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        accuracy = 100 * correct / len(train_loader.dataset)\n",
    "        v_accuracy = 100 * v_correct / len(val_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        stop_time = time.time()\n",
    "        print(f'Epoch: {epoch:03d}; Loss: {epoch_loss:0.6f}; Accuracy: {accuracy:0.4f}; Val Loss: {val_loss:0.6f}; Val Acc: {v_accuracy:0.4f}; Elapsed time: {(stop_time - start_time):0.4f}')\n",
    "        history['accuracy'].append(accuracy.cpu().numpy())\n",
    "        history['val_acc'].append(v_accuracy.cpu().numpy())\n",
    "        history['val_loss'].append(val_loss.cpu().detach().numpy())\n",
    "        history['loss'].append(epoch_loss.cpu().detach().numpy())\n",
    " \n",
    "        ###### Saving ######\n",
    "        if val_loss < best_val_loss:\n",
    "           \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model':model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "                },\n",
    "                f'{save_prefix}_best.pt')\n",
    "\n",
    "        if early_stopper(val_loss):\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    return(history)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "\n",
    "    # sets the model in evaluation mode.\n",
    "    # although our model does not have layers which behave differently during training and evaluation\n",
    "    # this is a good practice as the models architecture may change in the future\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "         \n",
    "        # forward pass, compute the output of the model for the current batch\n",
    "        outputs = model(images.to(device))\n",
    "\n",
    "        # \"max\" returns a namedtuple (values, indices) where values is the maximum \n",
    "        # value of each row of the input tensor in the given dimension dim; \n",
    "        # indices is the index location of each maximum value found (argmax).\n",
    "        # the argmax effectively provides the predicted class number        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct += (preds.cpu() == targets).sum()\n",
    "\n",
    "    return (correct / len(data_loader.dataset)).item()\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 3)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 3)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(32, 48, 3)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(48)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(48, 48, 3)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(48)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(1200, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):    \n",
    "        \n",
    "        # input = (bs, 3, 32, 32)\n",
    "        x = self.conv1(x) # -> (bs, 16, 26, 26)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x) # -> (bs, 32, 24, 24)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv3(x) # -> (bs, 48, 22, 22)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x) # -> (bs, 48, 20, 20)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = torch.flatten(x,1) # -> (bs, 48 * 20 * 20 = 1920)\n",
    "        x = self.fc1(x)        # -> (bs, 10)\n",
    "\n",
    "        return(x)\n",
    "\n",
    "\n",
    "\n",
    "class Early_Stopping():\n",
    "\n",
    "    def __init__(self, patience = 3, min_delta = 0.00001):\n",
    "\n",
    "        self.patience = patience \n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.min_delta\n",
    "        self.min_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        # improvement\n",
    "        if val_loss + self.min_delta < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "        # no improvement            \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def build_confusion_matrix(model, dataset):\n",
    "\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for images, targets in dataset:\n",
    "\n",
    "        predictions = model(images.to(device))\n",
    "        preds_sparse = [np.argmax(x) for x in predictions.cpu().detach().numpy()]\n",
    "        preds.extend(preds_sparse)\n",
    "        ground_truth.extend(targets.numpy())\n",
    "\n",
    "    vcpi_util.show_confusion_matrix(ground_truth, preds, len(test_set.classes))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST_SET = 'H:/vcpi/gtsrb/test'\n",
    "\n",
    "transform = v2.Compose(\n",
    "    [v2.Resize((32,32)), \n",
    "     v2.ToImage(), \n",
    "     v2.ToDtype(torch.float32, scale=True)]) \n",
    "\n",
    "# No shuffle is required for the test set, also the batch size can be completely different\n",
    "test_set = torchvision.datasets.ImageFolder(root=PATH_TEST_SET, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1200, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv(len(test_set.classes))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    reload = torch.load(f'aug_II_{i}_best.pt')\n",
    "    model = Conv(8)\n",
    "    model.load_state_dict(reload['model'])\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    acc.append(evaluate(models[i], test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948201358318329 0.9961630702018738 0.9930455684661865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99, 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "index = [i for i in range(10)]\n",
    "  \n",
    "plt.bar(index, acc)\n",
    "print(np.mean(acc), np.max(acc), np.min(acc))\n",
    "plt.ylim(0.99,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels_logits_and_preds(models):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = [[] for _ in range(10)]\n",
    "        labels = []\n",
    "\n",
    "        for images, labs in test_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels.extend(labs)\n",
    "            \n",
    "            for i in range(10):\n",
    "                logits[i].extend(models[i](images).cpu())\n",
    "\n",
    "\n",
    "    return labels, logits\n",
    "\n",
    "labels, logits = get_labels_logits_and_preds(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[] for _ in range(len(labels)) ]\n",
    "\n",
    "for index in range(len(labels)):\n",
    "    preds[index] = [np.argmax(logits[m][index].cpu().numpy()) for m in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(preds[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_sum_of_logits(logits):\n",
    "\n",
    "    sum_logits = []\n",
    "\n",
    "    for i in range(len(logits[0])):\n",
    "\n",
    "        log = logits[0][i]\n",
    "        for m in range(1, 10):\n",
    "            log = np.add(log, logits[m][i])\n",
    "        sum_logits.append(np.argmax(log))\n",
    "    return(sum_logits)\n",
    "    \n",
    "class_logits = get_class_from_sum_of_logits(logits)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(class_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  4170\n",
      "All correct:  4113\n",
      "All incorrect:  4\n",
      "Majority correct:  42\n",
      "Tie Vote:  2\n",
      "Majority Wrong:  9\n",
      "Percentage right:  0.9964028776978417\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "def get_stats(labels, class_preds, class_logits):\n",
    "\n",
    "    all_correct = 0\n",
    "    all_incorrect = 0\n",
    "    maj_vote = 0\n",
    "    maj_wrong = 0\n",
    "    tie = 0\n",
    "    count = 0\n",
    "\n",
    "    for k in range(len(labels)):\n",
    "\n",
    "        counter = collections.Counter(class_preds[k])\n",
    "        if len(counter) == 1:\n",
    "            if counter.most_common(1)[0][0] == labels[k]:\n",
    "                all_correct += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        else:\n",
    "            aux = counter.most_common(2)\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] == labels[k]:\n",
    "                maj_vote += 1\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] != labels[k]:\n",
    "                maj_wrong += 1\n",
    "            elif aux[0][1] == aux[1][1]:\n",
    "                tie += 1\n",
    "\n",
    "        count += 1 \n",
    "        \n",
    "    return [count, all_correct, all_incorrect, maj_vote, tie, maj_wrong]\n",
    "    \n",
    "    \n",
    "res = get_stats(labels, preds, class_logits)\n",
    "print('total: ', res[0])\n",
    "print('All correct: ', res[1])\n",
    "print('All incorrect: ', res[2])\n",
    "print('Majority correct: ', res[3])\n",
    "print('Tie Vote: ', res[4])\n",
    "print('Majority Wrong: ', res[5])\n",
    "print('Percentage right: ', (res[1]+res[3])/res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
